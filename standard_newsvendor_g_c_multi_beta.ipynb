{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency: general construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes to computation\n",
    "Let $x\\in\\mathcal{X}$, $z\\in\\mathcal{Z}$, $\\theta\\in\\Theta$ and recall notation\n",
    "<!-- $$\n",
    "    C(x,z) = \\max_{\\theta\\in\\Theta} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r),\\qquad\n",
    "    X(z) = \\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} \\max_{\\theta\\in\\Theta} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r)\\qquad\\text{and}\\qquad\n",
    "    u(z) = \\min_{x\\in\\mathcal{X}} \\max_{\\theta\\in\\Theta} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r).\n",
    "$$ -->\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    G(x,z) &= \\max_{\\theta\\in\\Theta} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r),\\\\\n",
    "    X(z) &= \\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} \\max_{\\theta\\in\\Theta} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r),\\\\\n",
    "    u(z) &= \\min_{x\\in\\mathcal{X}} \\max_{\\theta\\in\\Theta} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r),\\\\\n",
    "    \\varphi(x,\\theta) &= c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r) \\qquad\\text{and}\\\\\n",
    "    \\text{regret}(x,\\theta)&=c(x,\\theta) - \\min_{y\\in\\mathcal{X}} c(y,\\theta),\n",
    "\\end{align*}\n",
    "where $\\ell_{\\beta}^{-1}$ is specified later.\n",
    "We are primarely interested in the **consistency gap ($R_{\\infty}$)** and **regret in the $n$-th step ($R_n$)** \n",
    "\\begin{align*}\n",
    "    R_{\\infty} &= \\text{regret}(X(Z_{\\infty}(\\theta_0)),\\theta_0) \\qquad \\text{and}\\quad &R_{n}&=\\text{regret}(X(Z_{n}),\\theta_0).\\\\\n",
    "    &&&\n",
    "\\end{align*}\n",
    "    <!--,\\\\\n",
    "     & = c(X(\\theta_0),\\theta_0) +\\frac{1}{4\\rho}\\theta_0^{\\top}\\Sigma^{-1}\\theta_0 &&=c(X(Z_{n}),\\theta_0) +\\frac{1}{4\\rho}\\theta_0^{\\top}\\Sigma^{-1}\\theta_0 -->\n",
    "\n",
    "### $\\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} c(x,\\theta)$ unique \\& known in closed form\n",
    "\n",
    "Assume that $\\min_{x\\in\\mathcal{X}} c(x,\\theta)$ has a unique solution and that there is $\\nu\\colon\\theta\\to\\mathcal{X}$ such that $$\\nu(\\theta)=\\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} c(x,\\theta)$$ and $\\min_{x\\in\\mathcal{X}} c(x,\\theta) = c(\\nu(\\theta),\\theta)$.\n",
    "\n",
    "Then, in order to compute $X(Z_{\\infty}(\\theta))$ and $X(Z_{n})$ (or any $X(z)$), we can make use of the **Sion's Minimax Theorem** (*applies ***if*** $(x,\\theta)\\mapsto c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r))$ is jointly continous (finite valued !!), strictly convex in $x$ and concave in $\\theta$, while both $\\mathcal{X}$ and $\\Theta$ are convex and compact*) and ***Nash*** (the optimal points $x^{\\star}$ and $\\theta^{\\star}$ can be determined independently of each other). Then, for any $z\\in\\mathcal{Z}$ we have\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathcal{X}} \\max_{\\theta\\in\\Theta} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r) &= \\max_{\\theta\\in\\Theta} \\min_{x\\in\\mathcal{X}} c(x,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r) \\\\\n",
    "&= \\max_{\\theta\\in\\Theta} c(\\nu(\\theta),\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r) \\equiv \\max_{\\theta\\in\\Theta} \\varphi_{\\min}(\\theta) = - \\min_{\\theta\\in\\Theta} - \\varphi_{\\min}(\\theta)\n",
    "\\end{align*}\n",
    "and we can approximate $\\theta_{z}=\\underset{\\theta\\in\\Theta}{\\operatorname{arg\\,max\\ }} c(\\nu(\\theta),\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r)$ numerically, so that \n",
    "\\begin{align*}\n",
    "    X(z) &\\approx \\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} c(x,\\theta_{z}) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta_{z}}(z) -r) \\\\\n",
    "    &= \\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} c(x,\\theta_{z}) \\\\\n",
    "    &= \\nu(\\theta_{z}).\n",
    "\\end{align*}\n",
    "Moreover,\n",
    "$$\n",
    "\\text{regret}(x,\\theta)=c(x,\\theta) - c(\\nu(\\theta),\\theta).\n",
    "$$\n",
    "\n",
    "### $\\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} c(x,\\theta)$ UNknown in closed form\n",
    "Contrarily to the Gaussian example, closed form solutions for $\\min_{x\\in\\mathcal{X}} c(x,\\theta)$ are not at our disposal. Therefore, we will use a grid search algorithm. Hence, let $\\mathcal{X}_{grid}\\vcentcolon=(x_m)_{m=1,\\ldots,M}$ be some (equidistant) grid with $M$ elements build on the set $\\mathcal{X}$.\n",
    "\n",
    "Let $z\\in\\mathcal{Z}$. We denote $\\varphi_m(\\theta)\\vcentcolon = c(x_m,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r)$. In order to compute $X(z)$ (e.g. $X(Z_{\\infty}(\\theta))$ or $X(Z_{n})$), we approximate \n",
    "\\begin{align*}\n",
    "    \\theta_{z,m}&=\\underset{\\theta\\in\\Theta}{\\operatorname{arg\\,max\\ }} c(x_m,\\theta) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r)\\\\\n",
    "    &=\\underset{\\theta\\in\\Theta}{\\operatorname{arg\\,min\\ }} - c(x_m,\\theta) + \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta}(z) -r)\\\\\n",
    "    &= \\underset{\\theta\\in\\Theta}{\\operatorname{arg\\,min\\ }} - \\varphi_m(\\theta)\n",
    "\\end{align*}\n",
    "for all $m=1,\\ldots,M$, so that\n",
    "\\begin{align*}\n",
    "    X(z) &\\approx \\underset{x_m, \\ m=1,\\ldots,M}{\\operatorname{arg\\,min\\ }} c(x_m,\\theta_{z,m}) - \\ell_{\\beta}^{-1}(\\mathcal{I}_{\\theta_{z,m}}(z) -r).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of $\\ell_{\\beta}^{-1}$\n",
    "see respective fct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for comparison\n",
    "1. **SAA:** (assuming $c(x,Z_n)$ well-defined)\n",
    "$$X_{SAA}(Z_{n}) =\\underset{x\\in\\mathcal{X}}{\\operatorname{arg\\,min\\ }} c(x,Z_n)\\qquad \\text{with}\\qquad R_{SAA,n}=\\text{regret}(X_{SAA}(Z_{n}),\\theta_0).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm  # for color maps\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "## document specific cell imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT run select\n",
    "# all cell #SAME# across consistency_general up to choices\n",
    "\n",
    "# Plot 0: with gridX - $R_\\infty$ vs. $\\beta$ \n",
    "run_plot_0 = True\n",
    "# Plot 4: K with gridX - Parallel Processing (PP)\n",
    "run_plot_4 = True\n",
    "# Plot 6: ... free ... (NOT IN THIS FILE)\n",
    "run_plot_6 = True\n",
    "# Plot 10: withOUT gridX - $R_\\infty$ vs. $\\beta$ (NOT IN THIS FILE)\n",
    "run_plot_10 = False\n",
    "# Plot 14: K withOUT gridX - Parallel Processing (PP) (NOT IN THIS FILE)\n",
    "run_plot_14 = False\n",
    "\n",
    "run_plot_list = [run_plot_0, run_plot_4, run_plot_6, run_plot_10, run_plot_14]\n",
    "run_plot_numstr_list = [\"0\", \"4\", \"6\",\"10\",\"14\"]\n",
    "\n",
    "## INPUT type select\n",
    "\n",
    "# Is c replaced by regret? (if yes, set \"type_regret = True\" and keep the \"c\" function so that regret = c - min_x c)\n",
    "type_regret = False\n",
    "\n",
    "# Do we have to use grid_X? (if yes, set \"type_gridX = True\")\n",
    "type_gridX = run_plot_0 or run_plot_4 or run_plot_6 # <-- currently neccesary choice\n",
    "\n",
    "# Is \\min_x c(x,\\theta) known in closed form OR should it be comuted via X_grid? (if yes, set \"type_minc_closedform = True\")\n",
    "type_minc_closedform = False # not implemented\n",
    "\n",
    "# Is \\argmin_x c(x,\\theta) known in closed form? (if yes, set \"type_argminc_closedform = True\")\n",
    "type_argminc_closedform = not type_gridX # <-- currently neccesary choice\n",
    "\n",
    "type_list = [type_regret, type_minc_closedform, type_argminc_closedform, type_gridX]\n",
    "type_str_list = [\"type_regret\", \"type_minc_closedform\", \"type_argminc_closedform\", \"type_gridX\"]\n",
    "################################\n",
    "### End of INPUT ###############\n",
    "################################\n",
    "\n",
    "# safeguards\n",
    "if not all([isinstance(_, bool) for _ in run_plot_list]):\n",
    "    sys.exit(\"Warning: one of the run select variables is not bool.\")\n",
    "if not all([isinstance(_, bool) for _ in type_list]):\n",
    "    sys.exit(\"Warning: one of the run select variables is not bool.\")\n",
    "if type_minc_closedform:\n",
    "    sys.exit(\"safeguard: code not yet ready for ... = True\")\n",
    "if run_plot_0 or run_plot_4 or run_plot_6:\n",
    "    if not type_gridX:\n",
    "        sys.exit(\"Warning: if run_plot_0 or run_plot_4 or run_plot_6 True, then type_gridX = True necessary.\")\n",
    "    if run_plot_10 or run_plot_14:\n",
    "        sys.exit(\"Warning: run_plot_0 or run_plot_4 or run_plot_6 True is mutual exclusive with run_plot_10 or run_plot_14.\")\n",
    "if run_plot_10 or run_plot_14:\n",
    "    if type_gridX:\n",
    "        sys.exit(\"Warning: if run_plot_10 or run_plot_14 True, then type_gridX = False necessary.\")\n",
    "    if run_plot_0 or run_plot_4 or run_plot_6:\n",
    "        sys.exit(\"Warning: run_plot_0 or run_plot_4 or run_plot_6 True is mutual exclusive with run_plot_10 or run_plot_14.\")\n",
    "    if type_regret:\n",
    "        sys.exit(\"Warning: type_regret = True not implemented for run_plot_10 or run_plot_14.\")\n",
    "\n",
    "\n",
    "def print_select():\n",
    "    run_plot_str = \"\"\n",
    "    for j, run_plot_j, run_plot_namestr_j in zip(range(len(run_plot_list)), run_plot_list, run_plot_numstr_list):\n",
    "        if j == 0:\n",
    "            run_plot_str += \"run_plot_\"+run_plot_namestr_j+f\"={run_plot_j}\"\n",
    "        else: \n",
    "            run_plot_str += \"; run_plot_\"+run_plot_namestr_j+f\"={run_plot_j}\"\n",
    "    print(run_plot_str)\n",
    "    type_str = \"\"\n",
    "    for j, type_j, type_str_j in zip(range(len(type_list)), type_list, type_str_list):\n",
    "        if j == 0:\n",
    "            type_str += type_str_j+f\"={type_j}\"\n",
    "        else: \n",
    "            type_str += \"; \"+type_str_j+f\"={type_j}\"\n",
    "    print(type_str)\n",
    "    print(\"\")\n",
    "print_select()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sim setup \\& INPUT boxes (**run this section!**)\n",
    "\n",
    "In the following simulations\n",
    "* $\\theta_0$ denotes the true unknown parameter value.\n",
    "* $N$ is the maximal path length \n",
    "* $K$ is the number of paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT: Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT set data parameters and seed\n",
    "d = 8\n",
    "N = 800 \n",
    "K = 300\n",
    "\n",
    "eps_theta = 0.001\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "## INPUT generate/set theta_zero\n",
    "if d == 3:\n",
    "    theta_zero = np.array([0.25, 0.4, 0.35])\n",
    "elif d == 4:\n",
    "    theta_zero = np.array([0.2, 0.4, 0.05, 0.35])\n",
    "elif d == 5:\n",
    "    theta_zero = np.array([0.15, 0.07, 0.45, 0.05, 0.28])\n",
    "elif d == 6:\n",
    "    theta_zero = np.array([0.07, 0.12, 0.35, 0.1, 0.28,0.08])\n",
    "elif d == 8:\n",
    "    theta_zero = np.array([0.115, 0.115, 0.115, 0.125, 0.135, 0.135, 0.135, 0.125]) \n",
    "    k_theta_zero = 1\n",
    "    p_theta_zero = 1.65\n",
    "    rho_theta_zero = 0.0025\n",
    "elif d == 10:\n",
    "    theta_zero = np.array([0.04, 0.15, 0.12, 0.07, 0.09, 0.165, 0.03, 0.185, 0.05, 0.10])\n",
    "elif d == 15:\n",
    "    theta_zero = np.array([0.02, 0.062, 0.09, 0.07, 0.09, 0.125, 0.03, 0.115, 0.06, 0.09, 0.10, 0.07, 0.03, 0.028, 0.02]) \n",
    "    k_theta_zero = 0.5\n",
    "    p_theta_zero = 0.9\n",
    "    rho_theta_zero = 0.05*(0.05)\n",
    "else:\n",
    "    theta_zero = np.random.dirichlet(alpha=np.ones(d))\n",
    "\n",
    "print(np.sum(theta_zero))\n",
    "\n",
    "## INPUT define function, which generates one path of (Z_n)_n of length N\n",
    "def generate_path(seed_num=42):\n",
    "    np.random.seed(seed_num)\n",
    "\n",
    "    xi = np.random.choice(range(1, d + 1), size=N, p=theta_zero)\n",
    "    Z_path = []\n",
    "    for n in range(N):\n",
    "        unique, counts = np.unique(xi[0:(n+1)], return_counts=True)\n",
    "        Z_n = np.zeros(d)\n",
    "        for j,i in enumerate(unique):\n",
    "            Z_n[i-1] = counts[j]/(n+1)\n",
    "        Z_path += [Z_n]\n",
    "    return Z_path\n",
    "\n",
    "## INPUT define main title string\n",
    "dim_par_string = r\"DGP: $d$ = \"+f\"{d}\"+r\", $N$ = \"+f\"{N}\"+r\", $K$ = \"+f\"{K}\"+r\", $\\theta_{0}$ = \"+f\"{theta_zero}\"\n",
    "\n",
    "################################\n",
    "### End of INPUT ###############\n",
    "################################\n",
    "# rest cell #SAME# across consistency_general\n",
    "\n",
    "## generate K paths of (Z_n)_n of length N\n",
    "Z_path_K_N_list = []\n",
    "for k_index in range(K):\n",
    "    # np.random.seed(12+5*k_index)\n",
    "    \n",
    "    ## generate one path of (Z_n)_n of length N\n",
    "    Z_path_k = generate_path(seed_num = 12+5*k_index)\n",
    "    Z_path_K_N_list += [Z_path_k]\n",
    "\n",
    "## reversed path list for parallel processing (PP)\n",
    "Z_path_N_K_list = [list(tup) for tup in zip(*Z_path_K_N_list)]\n",
    "\n",
    "print(f'theta_zero = {theta_zero}')\n",
    "# print(f'xi = {xi}')\n",
    "# print(unique)\n",
    "# print(counts)\n",
    "# print(f'Z_path = {Z_path}')\n",
    "if N > 4 and K > 3:\n",
    "    print(f'Z_path_N_K_list[3][2] = {Z_path_N_K_list[3][2]}')\n",
    "\n",
    "print(f'dim_par_string = {dim_par_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Z_path_N_K_list)\n",
    "new_n_list = np.arange(1,N+1,1).tolist()\n",
    "\n",
    "## cut from the end\n",
    "# N_new = 24\n",
    "# print(len(Z_path_N_K_list))\n",
    "# Z_path_N_K_list = Z_path_N_K_list[N-N_new:]\n",
    "# new_n_list = new_n_list[N-N_new:] # for check\n",
    "# N = N_new\n",
    "\n",
    "## every n-th\n",
    "every_nth = 5 # 4 # so that first displayed n \\in {1,...,N} is going to be ``every_nth'' and then ``2*every_nth'', ``3*every_nth'', ...\n",
    "Z_path_N_K_list = ['dummy'] + Z_path_N_K_list\n",
    "new_n_list = ['dummy'] + new_n_list  # for check\n",
    "Z_path_N_K_list = Z_path_N_K_list[::every_nth]\n",
    "new_n_list = new_n_list[::every_nth] # for check\n",
    "Z_path_N_K_list = Z_path_N_K_list[1:]\n",
    "new_n_list = new_n_list[1:] # for check\n",
    "N = len(Z_path_N_K_list)\n",
    "\n",
    "## print checks\n",
    "print(f'new_n_list = {new_n_list}')\n",
    "print(f'len(new_n_list) = {len(new_n_list)}')\n",
    "print(f'len(Z_path_N_K_list) = {len(Z_path_N_K_list)}')\n",
    "print(f'N = {N}')\n",
    "print(Z_path_N_K_list)\n",
    "\n",
    "# np.linalg.norm(np.array([0.06, 0.16, 0.45, 0.05, 0.28]) - theta_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT: functions and other model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS set default model parameters ...\n",
    "# ... for cost function\n",
    "k_default = 2\n",
    "p_default = 3\n",
    "rho_default = 0.05\n",
    "\n",
    "alpha_three = 10 # 10\n",
    "\n",
    "# ... for penalty etc.\n",
    "beta_default = 1\n",
    "r_default = 0.5\n",
    "\n",
    "make_sure_scalar_is_on = True\n",
    "\n",
    "## INPUT functions\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "def buildin_rel_entr(z,theta):\n",
    "    '''returns I_\\theta(z)'''\n",
    "    return np.sum(rel_entr(z,theta))\n",
    "\n",
    "def sample_x_inital(seed_num = 42):\n",
    "    '''returns some inital value for optimization over x (could be deterministic)'''\n",
    "    if not seed_num == False:\n",
    "        np.random.seed(seed_num)\n",
    "    return np.random.uniform(0+0.45, d-0.45)\n",
    "\n",
    "def sample_theta_inital(seed_num = 42):\n",
    "    '''returns some inital value for optimization over theta (could be deterministic)'''\n",
    "    if not seed_num == False:\n",
    "        np.random.seed(seed_num)\n",
    "    # return np.random.dirichlet(alpha=np.ones(d))\n",
    "    return np.ones(d)*(1/d)\n",
    "\n",
    "def quadratic_ratefct(z,theta):\n",
    "    '''returns I_\\theta(z)'''\n",
    "    return np.dot(theta.T - z.T, theta - z)/2\n",
    "\n",
    "## INPUT mother functions inv penalties\n",
    "def def_inv_ell_explin(beta = beta_default, version = 2):\n",
    "    if version == 0:\n",
    "        '''MK'''\n",
    "        def inv_ell_explin(y):\n",
    "            return (np.exp(beta*(y)) - 1)/beta\n",
    "    if version == 1:\n",
    "        def inv_ell_explin(y):\n",
    "            return (np.exp(beta*(y)) - 1)/beta + y\n",
    "    if version == 2:\n",
    "        '''me'''\n",
    "        def inv_ell_explin(y):\n",
    "            return (np.exp(beta*(y)) - 1)/beta + y*beta\n",
    "    if version == 3:\n",
    "        def inv_ell_explin(y):\n",
    "            return (np.exp(beta*(y)) - 1)/beta + y*1\n",
    "    if version == 4:\n",
    "        '''log'''\n",
    "        def inv_ell_explin(y):\n",
    "            return (np.exp(beta*(y)) - 1)/beta + np.log(y+1)*beta\n",
    "    if version == 5:\n",
    "        def inv_ell_explin(y):\n",
    "            return (np.exp(beta*(y)) + y - 1)/beta\n",
    "    if version == 6:\n",
    "        '''betaID'''\n",
    "        def inv_ell_explin(y):\n",
    "            return y*beta\n",
    "    if version == 7:\n",
    "        '''ID'''\n",
    "        def inv_ell_explin(y):\n",
    "            return y\n",
    "    return inv_ell_explin\n",
    "\n",
    "## INPUT mother functions model\n",
    "def def_cost(k = k_default, p = p_default, rho = rho_default):\n",
    "    def cost(x,theta):\n",
    "        '''returns c(x,theta)'''\n",
    "        return np.dot((np.tile(k*(x) + rho*(x)**2, (d, 1)).T - p*np.minimum(np.outer(x,np.ones(d)),np.arange(1,d+1))),theta) # notOCE x can be a vector\n",
    "    return cost\n",
    "\n",
    "if type_gridX:\n",
    "    def def_min_c(cost, X_grid):\n",
    "        def min_c(theta):\n",
    "            '''returns \\min_{x_m \\in X_grid} c(x_m,theta)'''\n",
    "            return min(cost(X_grid,theta))\n",
    "        return min_c\n",
    "    \n",
    "    def def_regret(cost, X_grid):\n",
    "        ## with OCE and regret:\n",
    "        min_c = def_min_c(cost, X_grid)\n",
    "        def regret(x,theta):\n",
    "            return cost(x,theta) - min_c(theta)\n",
    "        return regret \n",
    "    \n",
    "if type_argminc_closedform and not type_gridX:\n",
    "    def def_argmin_c(rho = rho_default):\n",
    "        def argmin_c(theta):\n",
    "            '''returns \\argmin_x c(x,theta), hence argmin_c(theta) = \\nu(theta)'''\n",
    "            return np.dot(Sigma_inv, theta)/(2*rho)\n",
    "        return argmin_c\n",
    "\n",
    "\n",
    "title_string_cost = \" \" + f\"(notOCE)\"\n",
    "if type_regret:\n",
    "    title_string_cost += \"(c=regret)\"\n",
    "################################\n",
    "### End of INPUT ###############\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "### Fixed Background ###########\n",
    "################################\n",
    "# all cell #SAME# across consistency_general\n",
    "\n",
    "# default dummies\n",
    "def rate_fct_default(z,theta):\n",
    "    return 0\n",
    "\n",
    "def inv_ell_default(y):\n",
    "    return y\n",
    "\n",
    "## functions\n",
    "def print_progress_j(j, end_time_j, start_time_j, comment_str_j = \"\"):\n",
    "    elapsed_time = end_time_j - start_time_j\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = elapsed_time % 60\n",
    "    print(f\"End of j = {j}\"+comment_str_j+f\"; time required {minutes}m {seconds:.2f}s\")\n",
    "\n",
    "def print_progress_generel(name_timed_str, end_time, start_time, comment_str = \"\"):\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = elapsed_time % 60\n",
    "    print(\"End of \"+name_timed_str+comment_str+f\"; time required {minutes}m {seconds:.2f}s\")\n",
    "\n",
    "def print_list_until_constant(some_list, some_list_str, round_dec_num = False):\n",
    "    if isinstance(round_dec_num, int) and not isinstance(round_dec_num, bool):\n",
    "        some_list = [round(num, round_dec_num) for num in some_list]\n",
    "    if len(some_list) < 6:\n",
    "        print(some_list_str + f\" = {some_list}\")\n",
    "    else: \n",
    "        for j in range(5, len(some_list)):\n",
    "            if some_list[j] == some_list[j - 1]:\n",
    "                break\n",
    "        print(some_list_str + f\"[:{j+1}] = {some_list[:(j+1)]}\")\n",
    "\n",
    "def time_stamp():\n",
    "    time_stamp_str = time.strftime(\"%d%m%Y_%H%M%S\")\n",
    "    # print(time_stamp_str)\n",
    "    return time_stamp_str\n",
    "\n",
    "def minimize_constraints_bounds_none(objfct, initial_val, constraints_val = False, bounds_val = False):\n",
    "    if not constraints_val == False and not bounds_val == False:\n",
    "        result = minimize(objfct, initial_val, constraints = constraints_val, bounds = bounds_val)\n",
    "    elif constraints_val == False and not bounds_val == False:\n",
    "        result = minimize(objfct, initial_val, bounds = bounds_val)\n",
    "    elif bounds_val == False and not constraints_val == False:\n",
    "        result = minimize(objfct, initial_val, constraints = constraints_val)\n",
    "    else:\n",
    "        result = minimize(objfct, initial_val, method='BFGS')\n",
    "    return result\n",
    "\n",
    "def print_conclusion():\n",
    "    print(\"\")\n",
    "    print(\"Conclusion\")\n",
    "    print(f'time_stamp_str = {time_stamp()}')\n",
    "\n",
    "if type_gridX:\n",
    "    def max_varphi(theta_initial, neg_varphi, constraints_theta, bounds_theta):\n",
    "        '''returns \\max_\\theta \\varphi(\\theta)'''\n",
    "        result = minimize_constraints_bounds_none(neg_varphi, theta_initial, constraints_val = constraints_theta, bounds_val = bounds_theta)\n",
    "        return - result.fun\n",
    "\n",
    "    def compute_X_Z_inf_grid(X_grid, theta_zero, cost, beta, version, r, rate_fct, constraints_theta, bounds_theta):\n",
    "        '''returns X_Z_inf, so that R_\\infty = R(X_Z_inf)'''\n",
    "        X_Z_inf = None\n",
    "        max_varphi_m_smallest = np.inf\n",
    "        for m,x_m in enumerate(X_grid):\n",
    "            np.random.seed(42*m+42)\n",
    "            theta_initial = sample_theta_inital(seed_num = 42*m+42)\n",
    "\n",
    "            # via scipy.optimize.minimize\n",
    "            neg_varphi_m  = def_neg_varphi(x_m, theta_zero, cost, inv_ell = def_inv_ell_explin(beta = beta, version = version), r = r, rate_fct = rate_fct)\n",
    "            max_varphi_m = max_varphi(theta_initial, neg_varphi_m, constraints_theta, bounds_theta)\n",
    "\n",
    "            ### debug\n",
    "            if max_varphi_m == np.inf or max_varphi_m == - np.inf:\n",
    "                print(f\"WARNING: max_varphi_m = {max_varphi_m}; \"+f\"theta_initial = {theta_initial}; \"+f\"m = {m}\"+\"--> continue to m+1\")\n",
    "                continue\n",
    "            ###\n",
    "            if max_varphi_m < max_varphi_m_smallest:\n",
    "                X_Z_inf = x_m\n",
    "                max_varphi_m_smallest = max_varphi_m\n",
    "        if X_Z_inf is None:\n",
    "                sys.exit(\"Error: X_Z_inf is None.\")\n",
    "        return X_Z_inf\n",
    "\n",
    "    def compute_X_Z_n_grid(X_grid, k_index, Z_n, cost, beta, version, r, rate_fct, constraints_theta, bounds_theta):\n",
    "        '''returns X_Z_n, so that R_n = R(X_Z_n)'''\n",
    "        X_Z_n = None\n",
    "        max_varphi_m_smallest = np.inf\n",
    "        for m,x_m in enumerate(X_grid):\n",
    "            np.random.seed(42*m+42 + k_index)\n",
    "            theta_initial = sample_theta_inital(seed_num = 42*m+42 + k_index)\n",
    "\n",
    "            ## via scipy.optimize.minimize\n",
    "            neg_varphi_m  = def_neg_varphi(x_m, Z_n, cost, inv_ell = def_inv_ell_explin(beta = beta, version = version), r = r, rate_fct = rate_fct)\n",
    "            max_varphi_m = max_varphi(theta_initial, neg_varphi_m, constraints_theta, bounds_theta)\n",
    "\n",
    "            ### debug\n",
    "            if max_varphi_m == np.inf or max_varphi_m == - np.inf:\n",
    "                print(f\"WARNING: max_varphi_m = {max_varphi_m}; \"+f\"theta_initial = {theta_initial}; \"+f\"m = {m}; Z_n = {Z_n}\"+\"--> continue to m+1\")\n",
    "                continue\n",
    "            ###\n",
    "            if max_varphi_m < max_varphi_m_smallest:\n",
    "                X_Z_n = x_m\n",
    "                max_varphi_m_smallest = max_varphi_m\n",
    "        if X_Z_n is None:\n",
    "            sys.exit(\"Error: X_Z_n is None.\")\n",
    "        return X_Z_n\n",
    "\n",
    "if type_argminc_closedform and not type_gridX: # compute_theta_Z_ ...\n",
    "    def compute_theta_Z_inf(theta_zero, cost, argmin_c, beta, version, r, rate_fct, constraints_theta, bounds_theta):\n",
    "        np.random.seed(42 + 2)\n",
    "        theta_initial = sample_theta_inital(seed_num = 42 + 2)\n",
    "        theta_z = approx_argmin(def_neg_varphi_min(theta_zero, cost, argmin_c, inv_ell = def_inv_ell_explin(beta = beta, version = version), r = r, rate_fct = rate_fct), theta_initial, constraints_val = constraints_theta, bounds_val = bounds_theta)\n",
    "        return theta_z\n",
    "    \n",
    "    def compute_theta_Z_n(k_index, Z_n, cost, argmin_c, beta, version, r, rate_fct, constraints_theta, bounds_theta):\n",
    "        np.random.seed(42*k_index+42 + 2)\n",
    "        theta_initial = sample_theta_inital(seed_num = 42*k_index+42 + 2)\n",
    "        theta_z = approx_argmin(def_neg_varphi_min(Z_n, cost, argmin_c, inv_ell = def_inv_ell_explin(beta = beta, version = version), r = r, rate_fct = rate_fct), theta_initial, constraints_val = constraints_theta, bounds_val = bounds_theta)\n",
    "        return theta_z\n",
    "    \n",
    "## mother functions\n",
    "\n",
    "if type_gridX: # compute_minX_c_theta\n",
    "    def compute_minX_c_theta(cost, theta, constraints_x = False, bounds_x = False):\n",
    "        np.random.seed(d)\n",
    "        x_initial = sample_x_inital(seed_num = False)\n",
    "        def cost_x(x):\n",
    "                return cost(x,theta)\n",
    "        result = minimize_constraints_bounds_none(cost_x, x_initial, constraints_val = constraints_x, bounds_val = bounds_x)\n",
    "        minX_c_theta = result.fun\n",
    "        return minX_c_theta\n",
    "\n",
    "if type_argminc_closedform and not type_gridX: # approx_argmin\n",
    "    def approx_argmin(objfct, initial_val, constraints_val = False, bounds_val = False):\n",
    "        '''approximates the argmin of the function objfct'''\n",
    "        result = minimize_constraints_bounds_none(objfct, initial_val, constraints_val = constraints_val, bounds_val = bounds_val)\n",
    "        return result.x\n",
    "\n",
    "def make_sure_scalar(input_obj, is_on = make_sure_scalar_is_on):\n",
    "    if is_on:\n",
    "        if not isinstance(input_obj, (int, float, np.int64)):\n",
    "            # print(\"WARNING: make_sure_scalar: isinstance(input_obj, (int, float)) = False\")\n",
    "            try:\n",
    "                if isinstance(input_obj, np.ndarray) and np.shape(input_obj)[0] == 1:\n",
    "                    input_obj_scalar = input_obj[0]\n",
    "            except:\n",
    "                print(\"WARNING: make_sure_scalar: EXCEPT ACTIVATED\")\n",
    "\n",
    "            if isinstance(input_obj_scalar, (int, float)):\n",
    "                # print(\"make_sure_scalar: type succesfully changed to scalar: \"+f\"{input_obj_scalar}\")\n",
    "                return input_obj_scalar\n",
    "            else:\n",
    "                print(\"WARNING: make_sure_scalar: isinstance(input_obj, (int, float)) = False\")\n",
    "                print(\"WARNING: make_sure_scalar did not succeed!\")\n",
    "        else:\n",
    "            return input_obj\n",
    "    else:\n",
    "        return input_obj\n",
    "\n",
    "if type_gridX:\n",
    "    def def_R(cost, minX_c_theta_zero, theta_zero = theta_zero):\n",
    "        # if not type_regret:\n",
    "        def R(x):\n",
    "            '''returns c(x,\\theta_0) - \\min_y c(y,\\theta_0)'''\n",
    "            x = make_sure_scalar(x)\n",
    "            out_Rx = cost(x,theta_zero) - minX_c_theta_zero\n",
    "            ## make sure R is scalar if x is also scalar\n",
    "            out_Rx = make_sure_scalar(out_Rx)\n",
    "            return out_Rx\n",
    "        return R\n",
    "\n",
    "if type_argminc_closedform and not type_gridX:\n",
    "    def def_R(cost, argmin_c, theta_zero = theta_zero):\n",
    "        def R(x):\n",
    "            '''given x computes regret(x,theta_zero)'''\n",
    "            return cost(x,theta_zero) - cost(argmin_c(theta_zero),theta_zero)\n",
    "        # regret(x,theta_zero)\n",
    "        return R\n",
    "    \n",
    "if type_gridX:\n",
    "    def def_neg_varphi(x, z, cost, inv_ell = inv_ell_default, r = r_default, rate_fct = rate_fct_default):\n",
    "        def neg_varphi(theta):\n",
    "            '''returns -\\varphi(\\theta)_x in type_gridX = True context'''\n",
    "            return - cost(x,theta) + inv_ell(rate_fct(z,theta) - r)\n",
    "        return neg_varphi\n",
    "\n",
    "if type_argminc_closedform and not type_gridX:\n",
    "    def def_neg_varphi_min(z, cost, argmin_c, inv_ell=inv_ell_default, r = r_default, rate_fct = rate_fct_default):\n",
    "        def neg_varphi_min(theta):\n",
    "            '''returns -\\varphi(\\theta)_min in type_gridX = False context'''\n",
    "            return - cost(argmin_c(theta),theta) + inv_ell(rate_fct(z,theta) - r)\n",
    "        return neg_varphi_min\n",
    "\n",
    "if type_gridX: # def_approx_min_c_object\n",
    "    def def_approx_min_c_object(cost):\n",
    "        def approx_min_c_object(theta, x_initial, constraints_x = False, bounds_x = False):\n",
    "            '''\n",
    "            returns the \"minimize\"-result-object corresponding to the problem \\min_x c(x,\\theta)\n",
    "            if bounds = [(0, d)], then x is constrained to [0, d]\n",
    "            '''\n",
    "            def cost_x(x):\n",
    "                return cost(x,theta)\n",
    "            result = minimize_constraints_bounds_none(cost_x, x_initial, constraints_val = constraints_x, bounds_val = bounds_x)\n",
    "            return result\n",
    "        return approx_min_c_object\n",
    "\n",
    "## plot functions\n",
    "def create_title_string_final_gridX_GapBeta(title_string_invell, title_string_pars, X_grid):\n",
    "    return r\"$($\" + title_string_invell + r\"$)\\quad$\" + title_string_pars+ r\"$\\quad($\"+ f\"len(X_grid) = {len(X_grid)}\" + r\"$)$\" # + r\"$\\quad($\"+ title_string_PGD + r\"$)$\")\n",
    "\n",
    "def create_plot_GapBeta(num_r_vals, beta_vals, r_vals, R_infty_list_J, title_string_final, fig_size = (10, 8), r_value_in_red = False):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    colors = cm.viridis(np.linspace(0, 1, num_r_vals))\n",
    "    for j, color, r in zip(range(num_r_vals), colors, r_vals):\n",
    "        if r == r_value_in_red:\n",
    "            color = 'red'\n",
    "        plt.plot(beta_vals, R_infty_list_J[j], linestyle='-', color=color, label = r\"$r$ = \"+f\"{r}\")\n",
    "    plt.axhline(0, linestyle='--', color='black',linewidth=0.5)\n",
    "    plt.xlabel(r'$\\beta$')\n",
    "    plt.ylabel(r'$R_\\infty$')\n",
    "    plt.title(title_string_final)\n",
    "    plt.suptitle(dim_par_string+title_string_cost)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def create_title_string_final_gridX(title_string_invell, title_string_pars, X_grid, conf_percentage):\n",
    "    return r\"$($\" + title_string_invell + r\"$)\\quad$\" + title_string_pars+ r\"$\\quad($\"+ f\"len(X_grid) = {len(X_grid)}\" + r\"$)\\quad$($K$ used, conf \"f\"{conf_percentage}%, PP)\" # + r\"$\\quad($\"+ title_string_PGD + r\"$)$\"\n",
    "\n",
    "def create_title_string_final_nogridX(title_string_invell, title_string_pars, conf_percentage):\n",
    "    return r\"$($\" + title_string_invell + r\"$)\\quad$\" + title_string_pars+ r\"$\\quad$(no gridX Sion)\" + r\"$\\quad$($K$ used, conf \"f\"{conf_percentage}%, PP)\" # + r\"$\\quad($\"+ title_string_PGD + r\"$)$\"\n",
    "\n",
    "def create_plot_PP(num_vals, conf_int, R_SAAn_list, R_SAAn_upper_list, R_SAAn_lower_list, beta_vals, r_vals, R_n_list_J, R_n_upper_list_J, R_n_lower_list_J, R_infty_J, title_string_final, fig_size = (10, 8), n_min = 0, yscale_log = False):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    colors = cm.viridis(np.linspace(0, 1, num_vals))\n",
    "\n",
    "    # SAA\n",
    "    plt.plot(range(n_min+1, N+1),R_SAAn_list[n_min:], linestyle='-', color='r', label='$R_{SAA,n}$')\n",
    "    if conf_int:\n",
    "        plt.plot(range(n_min+1, N+1),R_SAAn_upper_list[n_min:], linestyle=':', color = 'r')\n",
    "        plt.plot(range(n_min+1, N+1),R_SAAn_lower_list[n_min:], linestyle=':', color = 'r')\n",
    "\n",
    "    # Our method\n",
    "    for j, beta, r, color in zip(range(num_vals),beta_vals, r_vals, colors):\n",
    "        plt.plot(range(n_min+1, N+1),R_n_list_J[j][n_min:], linestyle='-', color = color, label=r\"$R_{n},\\; \\beta$ = \"+f\"{beta}\"+r\", $r$ = \"+f\"{r}\")\n",
    "        if conf_int:\n",
    "            plt.plot(range(n_min+1, N+1),R_n_upper_list_J[j][n_min:], linestyle=':', color = color)\n",
    "            plt.plot(range(n_min+1, N+1),R_n_lower_list_J[j][n_min:], linestyle=':', color = color)\n",
    "        plt.axhline(R_infty_J[j], linestyle='--', color = color, label=r'$\\Gamma_{\\theta_0} = $'+f\"{R_infty_J[j]:.4f}\")\n",
    "\n",
    "    # plt.xticks(range(n_min+1, N+1)) # Ensure x-axis ticks show the integers 1, 2, 3, ...\n",
    "    if not yscale_log:\n",
    "        plt.axhline(0, linestyle='--', color='black',linewidth=0.5)\n",
    "    else:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.xlabel('$n$')\n",
    "    plt.ylabel('$R_n$')\n",
    "    plt.title(title_string_final+r'$\\quad$'+f'(n_min={n_min})')\n",
    "    plt.legend()\n",
    "    plt.suptitle(dim_par_string+title_string_cost)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT: Safeguards (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT safeguards\n",
    "# if any(element < 0.001 for element in theta_zero):\n",
    "#     sys.exit(\"Error: Make sure that theta_zero >= 0.001 elementwise.\")\n",
    "if not np.sum(theta_zero) == 1:\n",
    "    sys.exit(\"Error: np.sum(theta_zero) == 1 is False\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 0: with gridX - $R_\\infty$ vs. $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_plot_0:\n",
    "    print_select()\n",
    "    ## INPUT set parameter values ...\n",
    "    # ... fix single valued\n",
    "    k = k_theta_zero\n",
    "    p = p_theta_zero\n",
    "    rho = rho_theta_zero \n",
    "    title_string_pars = r\"$k$ = \"+f\"{k}\"+r\", $p$ = \"+f\"{p}\"+r\", $\\rho$ = \"+f\"{rho}\"\n",
    "\n",
    "    # ... multi-input\n",
    "    beta_vals = [0.1, 0.3, 0.5, 0.8] + np.arange(1,31,1).tolist() + np.arange(35,100+5,5).tolist()\n",
    "    r_vals = [0.01, 0.1, 0.5, 1, 10, 100]\n",
    "    r_value_in_red = 0.5\n",
    "\n",
    "    ## INPUT define the cost function\n",
    "    cost = def_cost(k = k, p = p, rho = rho)\n",
    "\n",
    "    ## INPUT define the grid\n",
    "    step_size = 0.05\n",
    "    alpha_two = d\n",
    "    X_grid = np.arange(0,alpha_two+step_size,step_size)\n",
    "    M = len(X_grid)\n",
    "\n",
    "    ## INPUT specify inv_ell_explin version\n",
    "    version = 2\n",
    "\n",
    "    ## INPUT set rate function\n",
    "    rate_fct = buildin_rel_entr\n",
    "    title_string_invell = r\"$\\ell^{-1}_{\\beta}\\; v.$\"+f\"{version}\"\n",
    "\n",
    "    ## INPUT specify scipy.optimize.minimize settings w.r.t. theta\n",
    "    constraints_theta = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "    bounds_theta = [(eps_theta, None)]*d\n",
    "\n",
    "    ## INPUT specify constraints for scipy.optimize.minimize w.r.t. x\n",
    "    constraints_x = False\n",
    "    bounds_x = [(0, d)]\n",
    "\n",
    "    ################################\n",
    "    ### End of INPUT ###############\n",
    "    ################################\n",
    "    # rest cell #SAME# across consistency_general\n",
    "\n",
    "    # num_beta_vals = len(beta_vals)\n",
    "    num_r_vals = len(r_vals)\n",
    "\n",
    "    # define R function\n",
    "    minX_c_theta_zero = compute_minX_c_theta(cost, theta_zero, constraints_x = constraints_x, bounds_x = bounds_x)\n",
    "    R = def_R(cost, minX_c_theta_zero, theta_zero = theta_zero)\n",
    "\n",
    "    # if type_regret = True: replace the cost by regret (note that cost_exec is the \"cost function\" used in all what follows)\n",
    "    if type_regret:\n",
    "        cost_exec = def_regret(cost, X_grid)\n",
    "    else:\n",
    "        cost_exec = cost\n",
    "\n",
    "    ## initialize lists for the J-loop\n",
    "    R_infty_list_J = [[] for _ in range(num_r_vals)]\n",
    "    for j, r in zip(range(num_r_vals), r_vals):\n",
    "        start_time_j = time.time()\n",
    "        for beta in beta_vals:\n",
    "            ## compute R_{\\infty}'s\n",
    "            X_Z_inf = compute_X_Z_inf_grid(X_grid, theta_zero, cost_exec, beta, version, r, rate_fct, constraints_theta, bounds_theta)\n",
    "            R_infty_j = R(X_Z_inf)\n",
    "            R_infty_list_J[j] += [R_infty_j]\n",
    "            \n",
    "        end_time_j = time.time()\n",
    "        print_progress_j(j,end_time_j, start_time_j, comment_str_j = f\" (r = {r})\")\n",
    "        print_list_until_constant(R_infty_list_J[j], f\"R_infty_list_J[{j}]\", round_dec_num = False)\n",
    "        print(\" \")\n",
    "\n",
    "    ## Plot\n",
    "    title_string_final = create_title_string_final_gridX_GapBeta(title_string_invell, title_string_pars, X_grid)\n",
    "    create_plot_GapBeta(num_r_vals, beta_vals, r_vals, R_infty_list_J, title_string_final, fig_size = (10, 8), r_value_in_red = r_value_in_red)\n",
    "\n",
    "    print_conclusion()\n",
    "\n",
    "else:\n",
    "    print(f\"run_plot_0 = {run_plot_0}; hence, cell not executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 4: K with gridX - Parallel Processing (PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_plot_4:\n",
    "    print_select()\n",
    "    from joblib import Parallel, delayed\n",
    "    from tqdm.notebook import tqdm #\n",
    "    from tqdm_joblib import tqdm_joblib #\n",
    "    print(dim_par_string)\n",
    "    print(\" \")\n",
    "\n",
    "    ## INPUT set number of parallel processses \n",
    "    PP_n_jobs = 6 # 4\n",
    "\n",
    "    ## INPUT set parameter values ...\n",
    "    # ... fix single valued\n",
    "    k = k_theta_zero \n",
    "    p = p_theta_zero \n",
    "    rho = rho_theta_zero \n",
    "    title_string_pars = r\"$k$ = \"+f\"{k}\"+r\", $p$ = \"+f\"{p}\"+r\", $\\rho$ = \"+f\"{rho}\"\n",
    "\n",
    "    # ... multi-input\n",
    "    beta_vals = [15.0, 25.0, 35.0, 80.0] \n",
    "    r_vals = [0.5]*len(beta_vals)\n",
    "\n",
    "    ## INPUT define the cost function\n",
    "    cost = def_cost(k = k, p = p, rho = rho)\n",
    "\n",
    "    ## INPUT define the grid\n",
    "    step_size = 0.2\n",
    "    alpha_two = d\n",
    "    X_grid = np.arange(0,alpha_two+step_size,step_size)\n",
    "    M = len(X_grid)\n",
    "\n",
    "    ## INPUT specify inv_ell_explin version\n",
    "    version = 2\n",
    "\n",
    "    ## INPUT set rate function\n",
    "    rate_fct = buildin_rel_entr \n",
    "    title_string_invell = r\"$\\ell^{-1}_{\\beta}\\; v.$\"+f\"{version}\"\n",
    "\n",
    "    ## INPUT specify scipy.optimize.minimize settings w.r.t. theta\n",
    "    constraints_theta = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "    bounds_theta = [(eps_theta, None)]*d \n",
    "\n",
    "    ## INPUT specify constraints for scipy.optimize.minimize w.r.t. x\n",
    "    constraints_x = False\n",
    "    bounds_x = [(0, d)]\n",
    "\n",
    "    ## INPUT confidence intervals\n",
    "    conf_int = True\n",
    "    conf_percentage = 5\n",
    "\n",
    "    ################################\n",
    "    ### End of INPUT ###############\n",
    "    ################################\n",
    "    # rest cell #SAME# across consistency_general\n",
    "\n",
    "    # safeguards\n",
    "    if not (len(beta_vals) == len(r_vals)):\n",
    "        sys.exit(\"Error: The variables beta_vals and r_vals must have the same length.\")\n",
    "    else:\n",
    "        num_vals = len(beta_vals)\n",
    "\n",
    "    # define R function\n",
    "    minX_c_theta_zero = compute_minX_c_theta(cost, theta_zero, constraints_x = constraints_x, bounds_x = bounds_x)\n",
    "    R = def_R(cost, minX_c_theta_zero, theta_zero = theta_zero)\n",
    "\n",
    "    # define other functions\n",
    "    approx_min_c_object = def_approx_min_c_object(cost)\n",
    "\n",
    "    # if type_regret = True: replace the cost by regret (note that cost_exec is the \"cost function\" used in all what follows (except SAA))\n",
    "    if type_regret:\n",
    "        cost_exec = def_regret(cost, X_grid)\n",
    "    else:\n",
    "        cost_exec = cost\n",
    "\n",
    "\n",
    "    #### SAA\n",
    "    start_time_SAA = time.time()\n",
    "    ## Define function for parallel processing (replacing the \"for n in range(N):\" loop\n",
    "    def compute_R_SAAns(Z_n_Klist):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Z_n_Klist: List of K samples for Z_n with fixed n.\n",
    "        \"\"\"\n",
    "        ## initialize lists for the K-loop\n",
    "        R_SAAn_Klist = []\n",
    "        for k_index in range(K):\n",
    "            Z_n = Z_n_Klist[k_index]\n",
    "            \n",
    "            ## SAA method: compute respective R_SAAn with z = Z_n\n",
    "            # np.random.seed(42+1000 + k_index)\n",
    "            # x_initial = sample_x_inital(seed_num = 42+1000 + k_index)\n",
    "            # result_SAA = approx_min_c_object(Z_n,x_initial, constraints_x = constraints_x, bounds_x = bounds_x)\n",
    "            # R_SAAn_Klist += [R(result_SAA.x)]\n",
    "\n",
    "            cost_evals = cost_exec(X_grid,Z_n) ##\n",
    "            min_index = np.argmin(cost_evals) ##\n",
    "            R_SAAn_Klist += [R(X_grid[min_index])] ##\n",
    "\n",
    "        R_SAAn = np.mean(R_SAAn_Klist)\n",
    "        R_SAAn_upper = np.percentile(R_SAAn_Klist, 100-conf_percentage)\n",
    "        R_SAAn_lower = np.percentile(R_SAAn_Klist, conf_percentage)\n",
    "\n",
    "        return R_SAAn_Klist, R_SAAn, R_SAAn_upper, R_SAAn_lower  # Multiple outputs as a tuple\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # Input data\n",
    "        inputs = Z_path_N_K_list  # Numbers to compute\n",
    "\n",
    "        # Use joblib for parallel processing\n",
    "        results = Parallel(n_jobs=PP_n_jobs)(\n",
    "            delayed(compute_R_SAAns)(list_n) for list_n in inputs\n",
    "        )\n",
    "\n",
    "        # Unpack the multiple outputs\n",
    "        R_SAAn_Klist_list, R_SAAn_list, R_SAAn_upper_list, R_SAAn_lower_list = zip(*results)\n",
    "\n",
    "        print(f\"R_SAAn_list[0:10] = {R_SAAn_list[0:10]}\")\n",
    "        print(f\"max(R_SAAn_list[0:10]) = {max(R_SAAn_list[0:10])}\")\n",
    "\n",
    "    end_time_SAA = time.time()\n",
    "    print_progress_generel(\"SAA\", end_time_SAA, start_time_SAA, comment_str = \"\")\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "    #### Our Method\n",
    "    ## initialize lists for the J-loop\n",
    "    R_infty_J = []\n",
    "    R_n_Klist_list_J = []\n",
    "    R_n_list_J = []\n",
    "    R_n_upper_list_J = []\n",
    "    R_n_lower_list_J = []\n",
    "\n",
    "    for j, beta, r in zip(range(num_vals),beta_vals, r_vals):\n",
    "        start_time_j = time.time()\n",
    "\n",
    "        ## compute R_{\\infty}'s\n",
    "        X_Z_inf = compute_X_Z_inf_grid(X_grid, theta_zero, cost_exec, beta, version, r, rate_fct, constraints_theta, bounds_theta)\n",
    "        R_infty = R(X_Z_inf)\n",
    "        R_infty_J += [R_infty]\n",
    "        print(f\"R_infty = {R_infty}\")\n",
    "\n",
    "        ## Define function for parallel processing (replacing the \"for n in range(N):\" loop)\n",
    "        def compute_R_ns(Z_n_Klist):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                Z_n_Klist: List of K samples for Z_n with fixed n.\n",
    "            \"\"\"\n",
    "            ## initialize lists for the K-loop\n",
    "            R_n_Klist = []\n",
    "\n",
    "            for k_index in range(K):\n",
    "                Z_n = Z_n_Klist[k_index]\n",
    "\n",
    "                ## approximate X_z with z = Z_n and compute respective R_n (with respective inv_ell)\n",
    "                X_Z_n = compute_X_Z_n_grid(X_grid, k_index, Z_n, cost_exec, beta, version, r, rate_fct, constraints_theta, bounds_theta)\n",
    "                R_n_Klist += [R(X_Z_n)]\n",
    "            \n",
    "            R_n = np.mean(R_n_Klist)\n",
    "            R_n_upper = np.percentile(R_n_Klist, 100-conf_percentage)\n",
    "            R_n_lower = np.percentile(R_n_Klist, conf_percentage)\n",
    "\n",
    "            return R_n_Klist, R_n, R_n_upper, R_n_lower  # Multiple outputs as a tuple\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            # Input data\n",
    "            inputs = Z_path_N_K_list  # Numbers to compute\n",
    "\n",
    "            with tqdm_joblib(tqdm(desc=\"Parallel Jobs\", total=len(inputs))) as progress_bar:\n",
    "                results = Parallel(n_jobs=PP_n_jobs)(\n",
    "                    delayed(compute_R_ns)(list_n) for list_n in inputs\n",
    "                )\n",
    "\n",
    "            # Unpack the multiple outputs\n",
    "            R_n_Klist_list, R_n_list, R_n_upper_list, R_n_lower_list = zip(*results)\n",
    "\n",
    "            R_n_Klist_list_J += [R_n_Klist_list]\n",
    "            R_n_list_J += [R_n_list]\n",
    "            R_n_upper_list_J += [R_n_upper_list]\n",
    "            R_n_lower_list_J += [R_n_lower_list]\n",
    "\n",
    "            print(f\"R_n_list[0:10] = {R_n_list[0:10]}\")\n",
    "            print(f\"max(R_n_list[0:10]) = {max(R_n_list[0:10])}\")\n",
    "\n",
    "        end_time_j = time.time()\n",
    "        print_progress_j(j,end_time_j, start_time_j, comment_str_j = f\" (beta = {beta}, r = {r})\")\n",
    "        print(\" \")\n",
    "\n",
    "\n",
    "    ## Plot\n",
    "    title_string_final = create_title_string_final_gridX(title_string_invell, title_string_pars, X_grid, conf_percentage)\n",
    "    create_plot_PP(num_vals, conf_int, R_SAAn_list, R_SAAn_upper_list, R_SAAn_lower_list, beta_vals, r_vals, R_n_list_J, R_n_upper_list_J, R_n_lower_list_J, R_infty_J, title_string_final, fig_size = (10, 8), n_min = 0)\n",
    "\n",
    "    print_conclusion()\n",
    "\n",
    "else:\n",
    "    print(f\"run_plot_4 = {run_plot_4}; hence, cell not executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot_PP(num_vals, True, R_SAAn_list, R_SAAn_upper_list, R_SAAn_lower_list, beta_vals, r_vals, R_n_list_J, R_n_upper_list_J, R_n_lower_list_J, R_infty_J, title_string_final, fig_size = (10, 8), n_min = 0, yscale_log=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
